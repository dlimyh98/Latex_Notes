\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Soft SVM - Introduction}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}What's wrong with the Hard SVM?}{1}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Single red outlier determines boundary - hallmark of overfitting}}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Developing a Soft SVM}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}Modifiying the classification constraint}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.2}Our new optimization goal now}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Multiclass SVM Loss - CS231N}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Recap on loss functions}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Visualizing linear classifiers}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Algebraic viewpoint}{2}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Weight matrix has it's own preference for each class, weighting the input image appropriately}}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Template viewpoint}{2}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Transform weight matrix into same dimension as input image, then do dot product  Recall that dot product is a measure of similarity}}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Visual viewpoint}{3}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Hyperplane seperating points (after kernel transformation)}}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Multiclass SVM Loss}{3}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Hinge Loss}}{3}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Example calculation}}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Gradient of Hinge Loss - Single datapoint}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Regularization}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.1}Intuition}{4}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Non-uniqueness of weights}}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Regularization term}{4}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Notice that $R(W)$ is function of weights only, not data  Intuitively speaking, smaller weights are preferred since they generalize better (no input dimension can have very large influence on scores all by itself)}}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Gradient of Hinge Loss - All datapoints}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.7.1}Vectorized Dot Product approach}{6}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces C (number of classes) = 3  D (data dimension) = 4  N (number of samples) = 3}}{6}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Dog class margin did not meet $\Delta $ constraint}}{6}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces No scaling required (as per derivation calculation)}}{6}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Final dW matrix for $L_{i}$ corresponding to cat image}}{6}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Convert \textit  {Loss} to a 'flag' matrix}}{7}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Strang's Row-Matrix multiplication idea}}{7}{}\protected@file@percent }
\gdef \@abspage@last{7}
